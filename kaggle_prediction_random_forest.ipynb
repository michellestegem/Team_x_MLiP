{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import re\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sales = pd.read_csv('./data/sales_train_validation.csv')\n",
    "sell_prices = pd.read_csv('./data/sell_prices.csv')\n",
    "calendar = pd.read_csv('./data/calendar.csv')\n",
    "submission_file = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsArray = (calendar['event_name_1'].append(calendar['event_name_2'])).unique()\n",
    "eventTypesArray = (calendar['event_type_1'].append(calendar['event_type_2'])).unique()\n",
    "weekDaysArray = calendar['weekday'].unique()\n",
    "states = train_sales['state_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayIndexString = 'd_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLastDayOfTrainset():\n",
    "    line2 = train_sales.columns[-1]\n",
    "    temp1 = re.findall(r'\\d+', line2) # through regular expression\n",
    "    return list(map(int, temp1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_nth_day(date, format='%Y%m%d'):\n",
    "    date = pd.to_datetime(date, format=format)\n",
    "    new_year_day = pd.Timestamp(year=date.year, month=1, day=1)\n",
    "    return (date - new_year_day).days + 1\n",
    "\n",
    "def eventsToDaysBefore(day_id, nmbr_days_in_future = 30):\n",
    "    events = np.full(eventsArray.size, nmbr_days_in_future)\n",
    "    events_types = np.full(eventTypesArray.size, nmbr_days_in_future)\n",
    "    nmbr_days_in_future = 30\n",
    "    for i in range(0, nmbr_days_in_future):\n",
    "        ## get day in future\n",
    "        idx = calendar.index[calendar['d'] == dayIndexString + str(day_id + i)]\n",
    "        dayDetails = calendar.loc[idx]\n",
    "        event_1 = dayDetails['event_name_1'].iloc[0]\n",
    "        # Process first event\n",
    "        if(not pd.isnull(event_1)):\n",
    "            index = np.where(eventsArray==event_1)\n",
    "            events[index] = i\n",
    "        event_2 = dayDetails['event_name_2'].iloc[0]\n",
    "\n",
    "        # Process second event\n",
    "        if(not pd.isnull(event_2)):\n",
    "            index = np.where(eventsArray==event_2)\n",
    "            events[index] = i\n",
    "\n",
    "        # Process first event type\n",
    "        event_t_1 = dayDetails['event_type_1'].iloc[0]\n",
    "        if(not pd.isnull(event_t_1)):\n",
    "            index = np.where(eventTypesArray==event_t_1)\n",
    "            events_types[index] = i\n",
    "\n",
    "        # Process second event type\n",
    "        event_t_2 = dayDetails['event_type_2'].iloc[0]\n",
    "        if(not pd.isnull(dayDetails['event_type_2'].iloc[0])):\n",
    "            index = np.where(eventTypesArray==event_t_2)\n",
    "            events_types[index] = i\n",
    "    return events, events_types\n",
    "    \n",
    "def getCalendarInfo(day_id):\n",
    "    idx = calendar.index[calendar['d'] == day_id]\n",
    "    dayDetails = calendar.loc[idx]\n",
    "    dayDetails['date'] =  date_to_nth_day(dayDetails['date'].iloc[0], '%Y-%m-%d')\n",
    "    # Remove weekday because table included wday value\n",
    "    dayDetails.drop(columns=['weekday'], inplace = True)\n",
    "    # Remove week id because I don't know what's the added value because we have year and number of day in year\n",
    "    dayDetails.drop(columns=['wm_yr_wk'], inplace = True)\n",
    "    \n",
    "    dayDetails.drop(columns=['event_name_1'], inplace = True)\n",
    "    dayDetails.drop(columns=['event_name_2'], inplace = True)\n",
    "    dayDetails.drop(columns=['event_type_1'], inplace = True)\n",
    "    dayDetails.drop(columns=['event_type_2'], inplace = True)\n",
    "    \n",
    "    # Reset year of 2011 to index year 0\n",
    "    dayDetails['year'] = dayDetails['year'] - 2011  \n",
    "    dayDetails['d'] = int(dayDetails['d'].str[2:])\n",
    "    return dayDetails\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainBatch(day_id = 1, day_range = 28):\n",
    "    train_container_panda = pd.DataFrame(index=train_sales.index)\n",
    "\n",
    "    for i in range(0, day_range):\n",
    "            # append sales of day\n",
    "            sales = train_sales[dayIndexString + str(day_id + i)]\n",
    "            train_container_panda['sale_' + str(i)] = sales\n",
    "    for i in range(0, 1):\n",
    "            # Get basic calendar info\n",
    "            calendarInfo = getCalendarInfo('d_' + str(day_id + day_range + i)).values[0]\n",
    "            # Get one-hot occur event on this day od\n",
    "            eventInfo = eventsToDaysBefore(day_id + day_range + i)\n",
    "            # Concatenate calendar info and event info\n",
    "            calendarInfo = np.concatenate((calendarInfo, eventInfo[0]))\n",
    "            # Concatenate info with event type info\n",
    "            calendarInfo = np.concatenate((calendarInfo, eventInfo[1]))\n",
    "            # append the event based information \n",
    "            columnIndex = 0\n",
    "            for col in calendarInfo:\n",
    "                train_container_panda['cal_' + str(day_id + day_range + i) + '_' + str(columnIndex)] = col\n",
    "                columnIndex = columnIndex + 1\n",
    "    return train_container_panda\n",
    "\n",
    "def testBatch(day_id = 1, day_range = 28):\n",
    "    train_container_panda = pd.DataFrame(index=train_sales.index)\n",
    "\n",
    "    for i in range(0, day_range):\n",
    "            # append sales of d|ay\n",
    "            print(dayIndexString + str(day_id + i))\n",
    "            sales = train_sales[dayIndexString + str(day_id + i)]\n",
    "            train_container_panda['sale_' + str(day_id + i)] = sales\n",
    "               \n",
    "    return train_container_panda\n",
    "\n",
    "def getDataset(numberOfDays):\n",
    "    lastTrainDay = getLastDayOfTrainset() + 1\n",
    "    \n",
    "    startIndex = lastTrainDay - numberOfDays;\n",
    "    return trainBatch(startIndex, numberOfDays).to_numpy()\n",
    "\n",
    "def getTestDataset(numberOfDays):\n",
    "    lastTrainDay = getLastDayOfTrainset() + 1\n",
    "    \n",
    "    startIndex = lastTrainDay - numberOfDays;\n",
    "    return testBatch(startIndex, numberOfDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('voting_batches.test', 'rb') as fid:\n",
    "    regr = pickle.load(fid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sumission_file(results, submission):\n",
    "    submision_data =  pd.DataFrame(submission_file['id'])\n",
    "\n",
    "    for col_index in range (1, len(results[0])+1):\n",
    "        submision_data['F' + str(col_index)] = 0\n",
    "    pred_data = pd.DataFrame(results)\n",
    "    pred_data.columns = submision_data.columns[1:]\n",
    "\n",
    "    submision_data.loc[pred_data.index] = pred_data\n",
    "    submision_data['id'] = submission_file['id']\n",
    "    submision_data.head()\n",
    "    submision_data.to_csv('final_submission_batch_all_lin.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = getDataset(56)\n",
    "prediction_results = regr.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sumission_file(prediction_results, submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_1886\n",
      "d_1887\n",
      "d_1888\n",
      "d_1889\n",
      "d_1890\n",
      "d_1891\n",
      "d_1892\n",
      "d_1893\n",
      "d_1894\n",
      "d_1895\n",
      "d_1896\n",
      "d_1897\n",
      "d_1898\n",
      "d_1899\n",
      "d_1900\n",
      "d_1901\n",
      "d_1902\n",
      "d_1903\n",
      "d_1904\n",
      "d_1905\n",
      "d_1906\n",
      "d_1907\n",
      "d_1908\n",
      "d_1909\n",
      "d_1910\n",
      "d_1911\n",
      "d_1912\n",
      "d_1913\n"
     ]
    }
   ],
   "source": [
    "original_results = getTestDataset(28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  0 ... 13 30  6]\n",
      " [ 0  0  0 ... 13 30  6]\n",
      " [ 0  0  0 ... 13 30  6]\n",
      " ...\n",
      " [ 1  2  2 ... 13 30  6]\n",
      " [ 2  2  2 ... 13 30  6]\n",
      " [ 2  0  5 ... 13 30  6]]\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8580054091231908"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(original_results, prediction_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8580054091231908"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(original_results, prediction_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.553889477615808"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.ones(np.shape(original_results)), prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = getDataset(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE 1.927 for batches of 56 of 10 estimators\n",
    "## RMSE 1.99 for batches of 56 of 1 estimators\n",
    "## RMSE 1.91 for retrain for each batch of 56 \n",
    "## RMSE 1.927 for batches of 56 of 10 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
